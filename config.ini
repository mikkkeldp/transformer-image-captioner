[vocab]
;path to file that contains captions
caption_path = ./dataset/Flickr8k.token.txt
; caption_path = ./dataset/new_caps.txt
;path to the training set image names
; train_path = ./dataset/Flickr_8k.trainImages.txt
train_path = ./dataset/Flickr_8k.new.txt
;path to where vocab .pkl file is to be saved
vocab_path = ./vocab.pkl
;minimum word frequency threshold
threshold = 1

[train]
;path for saving trained models
model_path = ./trained_models/
;path for vocabulary wrapper
vocab_path = ./vocab.pkl
;directory for images
image_dir = ./dataset/Flickr8k_Dataset
;path for caption file
caption_path = ./dataset/Flickr8k.token.txt
; caption_path = ./dataset/new_caps.txt
;path for train split file'
; train_path = ./dataset/Flickr_8k.trainImages.txt
train_path = ./dataset/Flickr_8k.new.txt
;path for validation split file
val_path = ./dataset/Flickr_8k.devImages.txt
;input image size for particular pretrained CNN (244/244/299) 224 for efficientNet
image_size = 244
;step size for printing log info
log_step = 20
;step size for saving trained models
save_step = 1000

;cnn used for feature extraction (resnet, vgg, inception,efficient)
cnn = resnet
;dimension of extracted features (1024, 512, 192) 2048 for panoptic, 2560 for efficient
encoder_size = 1024
;dimension of word embedding vectors (256,200)
embed_size = 256
;dimension of encoded image (8 for efficient)
encoded_image_size = 14
;dimension of attention layers
attention_size = 384
;dimension of lstm hidden states
hidden_size = 384
;use pre-trained embedding layer
glove = False
;number of epochs
num_epochs = 10
;batch size
batch_size = 64
;number of parallel workers during training
num_workers = 2
;learning rate for optimizer
learning_rate = 0.0005
;start training from scratcsh
scratch_training = True    
;starting epoch (if not trained from scratch)
starting_epoch = 10


; MY PROPOSALS
; Set attention regions: 
; None - indicates only high level attention regions to be used
; fa - indicates the low level attention regions of Faster R-CNN will be incorporated
; pa - indicates the low level attention regions of PanopticFPN will be incorporated
at = pa

; Augmented caps
aug_cap = False

[eval]
;path for trained encoder
encoder_path = ./trained_models/encoder-8.ckpt
;path for trained decoder
decoder_path = ./trained_models/decoder-8.ckpt
;path to vocabulary wrapper
vocab_path = ./vocab.pkl
;path to dataset images
image_dir = ./dataset/Flickr8k_Dataset
;path to file that contains captions
caption_path = ./dataset/Flickr8k.token.txt
; caption_path = ./dataset/new_caps.txt
;path for val split file
val_path = ./dataset/Flickr_8k.devImages.txt
;input image size for particular pretrained CNN (224 for vgg and resnet, 244 for everything else)
image_size = 244
;cnn used for feature extraction (resnet, vgg, inception)
cnn = resnet
;dimension of extracted features (1024, 512, 1280)
encoder_size = 1024
;dimension of word embedding vectors (256,200)
embed_size = 256
;dimension of encoded image
encoded_image_size = 14
;dimension of attention layers
attention_size = 384
;dimension of lstm hidden states
hidden_size = 384
;use pre-trained embedding layer
glove = False
;batch size
batch_size = 1
;number of parallel workers during training
num_workers = 0
;number of parallel beams 
beam_size = 8

; MY PROPOSALS
; Set attention regions: 
; None - indicates only high level attention regions to be used
; fa - indicates the low level attention regions of Faster R-CNN will be incorporated
; pa - indicates the low level attention regions of PanopticFPN will be incorporated
at = pa

;Enable or disable language model rescoring during beam search
language_rescoring = True
;Enable or disable object reinforcement during beam search
object_reinforce = False


[sample]
;path for trained encoder
encoder_path = ./trained_models/encoder-8.ckpt
;path for trained decoder
decoder_path = ./trained_models/decoder-8.ckpt
;path to vocabulary wrapper
vocab_path = ./vocab.pkl
;path to dataset images
image_dir = ./dataset/Flickr8k_Dataset
;path to file that contains captions
; caption_path = ./dataset/Flickr8k.token.txt
caption_path = ./dataset/new_caps.txt
;path for val split file
val_path = ./dataset/Flickr_8k.devImages.txt
;input image size for particular pretrained CNN
image_size = 244

;cnn used for feature extraction (resnet, vgg, inception)
cnn = resnet
;dimension of extracted features (1024, 512)
encoder_size = 1024
;dimension of word embedding vectors (256,200)
embed_size = 256
;dimension of encoded image
encoded_image_size = 14
;dimension of attention layers
attention_size = 384
;dimension of lstm hidden states
hidden_size = 384
;use pre-trained embedding layer
glove = False
;batch size
batch_size = 1
;number of parallel workers during training
num_workers = 0
;number of parallel beams 
beam_size = 15
;sample image
image = ./samples/364213568_7f83e7d144.jpg


at = pa

;Enable or disable language model rescoring during beam search
language_rescoring = True
;Enable or disable object reinforcement during beam search
object_reinforce = False



; Show attend and tell model parameters
; hidden size: 1024
; embed_size = 512
; epochs = 10
; batch_size = 100
; lr = 0.01
; optimizer adam



; baseline paramers
; hidden size: 384
; embed_size = 256
; attention_size = 384

; epochs = 10
; batch_size = 100
; lr = 0.0005
; optimizer adam
